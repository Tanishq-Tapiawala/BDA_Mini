{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3f99ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, timedelta, datetime\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c54fb399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark \n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.functions import * \n",
    "from pyspark.sql.types import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91ee8645",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/12 02:17:43 WARN Utils: Your hostname, Tanishqs-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.29.164 instead (on interface en0)\n",
      "22/04/12 02:17:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/12 02:17:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/04/12 02:17:44 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Create spark session\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Classification\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8064298b",
   "metadata": {},
   "source": [
    "# Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eda5b755",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[age: int, workClass: string, fnlwgt: int, education: string, education-num: int, marital-status: string, occupation: string, relationship: string, race: string, sex: string, capital-gain: int, capital-loss: int, hours-per-week: int, native-country: string, income: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\") \\\n",
    "  .option(\"inferSchema\", \"true\") \\\n",
    "  .option(\"header\", \"False\") \\\n",
    "  .option(\"sep\", \",\") \\\n",
    "  .load(\"census.csv\") \\\n",
    "  .toDF(\"age\", \"workClass\", \"fnlwgt\", \"education\", \"education-num\",\"marital-status\", \"occupation\", \"relationship\",\n",
    "        \"race\", \"sex\", \"capital-gain\", \"capital-loss\", \"hours-per-week\", \"native-country\", \"income\")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac5527a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+------+---------+-------------+------------------+---------------+-------------+-----+----+------------+------------+--------------+--------------+------+\n",
      "|age|       workClass|fnlwgt|education|education-num|    marital-status|     occupation| relationship| race| sex|capital-gain|capital-loss|hours-per-week|native-country|income|\n",
      "+---+----------------+------+---------+-------------+------------------+---------------+-------------+-----+----+------------+------------+--------------+--------------+------+\n",
      "| 39|       State-gov| 77516|Bachelors|           13|     Never-married|   Adm-clerical|Not-in-family|White|Male|        2174|           0|            40| United-States| <=50K|\n",
      "| 50|Self-emp-not-inc| 83311|Bachelors|           13|Married-civ-spouse|Exec-managerial|      Husband|White|Male|           0|           0|            13| United-States| <=50K|\n",
      "+---+----------------+------+---------+-------------+------------------+---------------+-------------+-----+----+------------+------------+--------------+--------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "977a0500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is having 48842 rows and 15 columns\n"
     ]
    }
   ],
   "source": [
    "# Data specs\n",
    "\n",
    "print('Data is having', df.count(), \"rows and\", len(df.columns), 'columns' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917d3c5b",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "780f5fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Create add new column to the dataset\n",
    "df = df.withColumn('>50K', F.when(df.income == '<=50K', 0).otherwise(1))\n",
    "\n",
    "# Drop the Income label\n",
    "df = df.drop('income')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1000ba7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age',\n",
       " 'workClass',\n",
       " 'fnlwgt',\n",
       " 'education',\n",
       " 'education-num',\n",
       " 'marital-status',\n",
       " 'occupation',\n",
       " 'relationship',\n",
       " 'race',\n",
       " 'sex',\n",
       " 'capital-gain',\n",
       " 'capital-loss',\n",
       " 'hours-per-week',\n",
       " 'native-country',\n",
       " '>50K']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbc0b043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting categorical features\n",
    "\n",
    "categorical_columns = [\n",
    " 'workClass',\n",
    " 'education',\n",
    " 'marital-status',\n",
    " 'occupation',\n",
    " 'relationship',\n",
    " 'race',\n",
    " 'sex',\n",
    " 'hours-per-week',\n",
    " 'native-country',\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70614ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.classification import (DecisionTreeClassifier, GBTClassifier, RandomForestClassifier)\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c))\n",
    "    for c in categorical_columns]\n",
    "\n",
    "# The encode of indexed values multiple columns\n",
    "encoders = [OneHotEncoder(dropLast=False,inputCol=indexer.getOutputCol(),\n",
    "            outputCol=\"{0}_encoded\".format(indexer.getOutputCol())) \n",
    "    for indexer in indexers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea17a290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing encoded values\n",
    "\n",
    "categorical_encoded = [encoder.getOutputCol() for encoder in encoders]\n",
    "numerical_columns = ['age', 'education-num', 'capital-gain', 'capital-loss']\n",
    "inputcols = categorical_encoded + numerical_columns\n",
    "assembler = VectorAssembler(inputCols=inputcols, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91ba3068",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Setting up a pipeline to automize the stages\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + encoders+[assembler])\n",
    "model = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28d4f0d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: int, workClass: string, fnlwgt: int, education: string, education-num: int, marital-status: string, occupation: string, relationship: string, race: string, sex: string, capital-gain: int, capital-loss: int, hours-per-week: int, native-country: string, >50K: int, workClass_indexed: double, education_indexed: double, marital-status_indexed: double, occupation_indexed: double, relationship_indexed: double, race_indexed: double, sex_indexed: double, hours-per-week_indexed: double, native-country_indexed: double, workClass_indexed_encoded: vector, education_indexed_encoded: vector, marital-status_indexed_encoded: vector, occupation_indexed_encoded: vector, relationship_indexed_encoded: vector, race_indexed_encoded: vector, sex_indexed_encoded: vector, hours-per-week_indexed_encoded: vector, native-country_indexed_encoded: vector, features: vector]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Transform data\n",
    "\n",
    "transformed = model.transform(df)\n",
    "display(transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "022ea249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the final dataset\n",
    "\n",
    "final_data = transformed.select('features', '>50K')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cbcafb",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "143a7660",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier(labelCol='>50K', featuresCol='features')\n",
    "\n",
    "rfc = RandomForestClassifier(numTrees=200, labelCol='>50K', featuresCol='features')\n",
    "\n",
    "gbt = GBTClassifier(labelCol='>50K', featuresCol='features', maxIter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c836c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39005\n",
      "9837\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data (80/20)\n",
    "\n",
    "train_data, test_data = final_data.randomSplit([0.8,0.2])\n",
    "print(train_data.count())\n",
    "print(test_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e74468a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/12 02:18:50 WARN DAGScheduler: Broadcasting large task binary with size 1169.7 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Model Training\n",
    "\n",
    "dtc_model = dtc.fit(train_data)\n",
    "rfc_model = rfc.fit(train_data)\n",
    "gbt_model = gbt.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8683250d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "\n",
    "dtc_preds = dtc_model.transform(test_data)\n",
    "rfc_preds = rfc_model.transform(test_data)\n",
    "gbt_preds = gbt_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c6eaba2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Performance using ROC\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='>50K')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25a96036",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 354:============================>                            (1 + 1) / 2]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTC => 0.5863205435814941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/12 02:19:00 WARN DAGScheduler: Broadcasting large task binary with size 1120.1 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFC => 0.8995973523009749\n",
      "GBT => 0.9118919812413948\n"
     ]
    }
   ],
   "source": [
    "print('DTC =>', evaluator.evaluate(dtc_preds))\n",
    "print('RFC =>', evaluator.evaluate(rfc_preds))\n",
    "print('GBT =>', evaluator.evaluate(gbt_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f9b003",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
